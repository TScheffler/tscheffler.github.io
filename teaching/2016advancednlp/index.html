<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
<html>
<head>
   <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <meta name="Author" content="Tatjana Scheffler">
   <title>Advanced NLP, winter semester 2016/2017</title>
   <meta name="description" content="Tatjana Scheffler - Twitterseminar">
   <link href="../../tshp.css" rel="stylesheet" type="text/css" />

</head>
<body>

<div  id="container">
<h1>Advanced Natural Language Processing</h1>

<p>
  <a href="#schedule">Schedule</a> 
</p>

<h2>Course organization</h2>

<p>
<table cellspacing="10">
  <TR>
    <TD width="100">Instructor:</TD>
    <TD><a href="http://www.ling.uni-potsdam.de/~scheffler/">Tatjana
      Scheffler</a></TD>
  </TR>
  <TR valign="top">
    <TD>TA:</TD>
    <TD>Jens Johannsmeier</TD>
  </TR>
  <TR valign="top">
    <TD>Time:</TD>
    <TD>Tuesdays, 12-2 p.m. <br />Fridays, 10 a.m.-12 </TD>
  </TR>
   <TR>
    <TD>Place:</TD>
    <TD>Golm, building 14, room 009</TD>
  </TR>
   <TR>
    <TD>Modules:</TD>
    <TD>CSBM1 (M.Sc. Cognitive Systems)</TD>
  </TR>
   <TR>
    <TD>Moodle:</TD>
    <TD>Please register on the course's <a href="https://moodle2.uni-potsdam.de/course/view.php?id=11449" >Moodle</a> site.</TD>
  </TR>
</table>
</p>

<h2>Requirements</h2>
<ul>
<li>regular readings</li>
<li>active participation</li>
<li>completion of the assignments: At least 5 (out of 6) assignments
must be turned in. We will add up the four best scores. At least 250
(out of 400) total points are required to pass the
course.<br/>
I will not accept late assignments. 10% will be deducted for each day
your assignment is late.</li>
</ul>
<h2>Grading policy:</h2>
<p>
The grade will be based on a <b>final project</b>, to be completed during the
semester break. The grade takes the following criteria into account:
proposal and its presentation, difficulty/creativity, correctness/performance,
code readability/documentation.
</p>

<h2>Course description</h2>

<p>
This class is the graduate-level introduction to computational
linguistics, a first-year class in the MSc Cognitive Systems. The
purpose of this class is to introduce the important concepts, models
and methods used in natural language processing (NLP). After the successful
completion of this course, students should be able to (i) read and
understand the scientific literature in the area of computational
linguistics and (ii) start implementing their own NLP projects.
</p>
<p>
We will cover the following topics:
</p>
<ul>
  <li>statistical models of language</li>
  <li>part of speech tagging (HMMs)</li>
  <li>syntactic parsing (PCFGs, others?)</li>
  <li>semantics</li>
  <li>machine translation</li>
  <li>speech processing</li>
  <li>and more</li>
</ul>

<h2><a id="schedule" >Schedule</a></h2>

</div>
<div id="bigtable">
<table frame="void" rules="none" width="100%">
<tbody>
<tr>
<th>Date</td>
<th>Topic</td>
<th>Readings</td>
<th>Assignments</td>
</tr>
<tr>
<td>T 10/18</td>
<td><a href="slides/01-introduction.pdf" >Introduction</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>F 10/21</td>
<td><a href="slides/02-probability.pdf" >Review: Probability Theory</a></td>
<td>Harald Goldstein, <a
  href="http://folk.uio.no/nhfehr/Goldstein%20-%20Introduction%20to%20Probability%20Theory.pdf"
  >A short introduction to probability and related concepts</a><br />
And/Or: Manning/Sch端tze, chapter 2.1<br />
Optional:  Kevin Murphy, <a href="http://www.cs.ubc.ca/~murphyk/Teaching/CS340-Fall06/reading/bernoulli.pdf" >Binomial and multinomial distributions</a></td>
<td><a href="assignments/anlp16-assignment-1.pdf" >A1 released</a></td>
</tr>
<tr>
<td>T 10/25</td>
<td><i>practical concerns</i> (TA)</td>
<td><a href="slides/anlp_notes_1.pdf" >notes for assignment</a></td>
<td></td>
</tr>
<tr>
<td>F 10/28</td>
<td><a href="slides/03-ngrams.pdf" >N-grams</a></td>
<td>Jurafsky/Martin, chapters 4.1-4.2 <br /><a href="https://www.youtube.com/watch?v=fCn8zs912OE" >A video about Zipf's law</a></td>
<td></td>
</tr>
<tr>
<td>T 11/1</td>
<td><a href="slides/04-smoothing.pdf" >Smoothing</a></td>
<td>Jurafsky/Martin, chapters 4.5-4.7<br />
Opt.: Manning/Sch端tze, chapters 6.2-6.3 </td>
<td></td>
</tr>
<tr>
<td>F 11/4</td>
<td><a href="https://moodle2.uni-potsdam.de/course/view.php?id=11449" >Part of Speech Tagging</a></td>
<td>Jurafsky/Martin, chapters 5.1-5.5, 6.1-6.4 </td>
<td>A1 due, <a href="assignments/anlp16-assignment-2.pdf" >A2 released</a></td>
</tr>
<tr>
<td>T 11/8</td>
<td><a href="https://moodle2.uni-potsdam.de/course/view.php?id=11449" >HMM Training</a></td>
<td>Jurafsky/Martin, chapters 5.7-5.8, 6.5 <br />
<a href="http://www.cs.jhu.edu/~jason/465/PowerPoint/lect24-hmm.xls" >HMM spreadsheet</a>, Eisner's ice cream example</td>
<td></td>
</tr>
<tr>
<td>F 11/11</td>
<td><i>discussion of A1</i></td>
<td><span style="color: #2288aa;">be prepared to present your solutions!</span></td>
<td></td>
</tr>
<tr>
<td>T 11/15</td>
<td>Context-Free Grammars & Parsing (CKY)</td>
<td>Jurafsky/Martin, chapter 13 (-13.4), <br /> J/M chapter 12 as background</td>
<td></td>
</tr>
<tr>
<td>F 11/18</td>
<td>Probabilistic Context-Free Grammars</td>
<td>Jurafsky/Martin, chapters 14.1-14.5, 14.7</td>
<td>A2 due, <a href="assignments/anlp16-assignment-3.pdf" >A3 released</a></td>
</tr>
<tr>
<td>T 11/22</td>
<td>Training PCFGs</td>
<td>Jurafsky/Martin, chapter 14.3<br />
Manning/Sch端tze, chapter 11<br />
Michael Collins, <a href="http://www.cs.columbia.edu/~mcollins/io.pdf" >The inside-outside algorithm.</a> </td>
<td></td>
</tr>
<tr>
<td>F 11/25</td>
<td><i>discussion of A2</i></td>
<td><span style="color: #2288aa;">be prepared to present your solutions!</span></td>
<td></td>
</tr>
<tr>
<td>T 11/29</td>
<td>Advanced PCFG models</td>
<td>Mark Johnson (1998), <a
  href="http://www.aclweb.org/anthology/J/J98/J98-4004.pdf" >PCFG
  Models of Linguistic Tree Representations </a>(esp. on parent
  annotations)<br />
Michael Collins, <a
  href="http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/lexpcfgs.pdf"
  >Lexicalized PCFGs</a><br />
Dan Klein/Chris Manning (2003), <a href="http://www.aclweb.org/anthology/P/P03/P03-1054.pdf" >Accurate unlexicalized parsing</a>
</td>
<td></td>
</tr>
<tr>
<td>F 12/2</td>
<td>Dependency parsing</td>
<td>McDonald/Pereira/Ribarov/Hajic (2005),
  <a href="http://www.aclweb.org/anthology/H05-1066" >Non-projective
  Dependency Parsing using Spanning Tree Algorithms</a><br />
Joakim Nivre (2008), <a href="http://www.aclweb.org/anthology/J08-4003" >Algorithms for Deterministic Incremental Dependency Parsing</a> </td>
<td>A3 due, <a href="assignments/anlp16-assignment-4.pdf" >A4 released</a></td>
</tr>
<tr>
<td>T 12/6</td>
<td>Speech recognition</td>
<td>Jurafsky/Martin, ch. 9<br />
Links for further reading: <br />
<a href="https://arxiv.org/abs/1610.05256" >"Human parity" speech recognition</a><br />
<a href="http://languagelog.ldc.upenn.edu/nll/?p=28894" >Language log on human parity speech recognition</a><br />
<a href="https://www.google.com/intl/en/chrome/demos/speech.html" >Google speech API web demo</a><br />
<a href="http://sebastian.germes.in/blog/2011/09/googles-speech-api/" >S. Germesin on using the speech API remotely</a></td>
<td></td>
</tr>
<tr>
<td>F 12/9</td>
<td><i>discussion of A3</i></td>
<td><span style="color: #2288aa;">be prepared to present your
  solutions!</span><br/>
Joshua Goodman, <a href="http://www.aclweb.org/anthology/J99-4004"
  >Semiring Parsing</a>. Computational Linguistics, 1999.</td>
<td></td>
</tr>
<tr>
<td>T 12/13</td>
<td>Speech synthesis</td>
<td>Jurafsky/Martin, ch. 8</td>
<td></td>
</tr>
<tr>
<td>F 12/16</td>
<td><span style="color: #ff0000;">no class</span></td>
<td></td>
<td>A4 due</td>
</tr>
<tr>
<td>
12/19-23</p>
</td>
<td><span style="color: #ff0000;">no class (winter break)</span></td>
<td></td>
<td></td>
</tr>
<tr>
<td>
12/26-30</p>
</td>
<td><span style="color: #ff0000;">no class (winter break)</span></td>
<td></td>
<td></td>
</tr>
<tr>
<td>T 1/3</td>
<td>Statistical machine translation: alignments</td>
<td>Jurafsky/Martin, ch. 25 (through 25.6)<br />
Adam Lopez, <a
  href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.421.5497&rep=rep1&type=pdf"
  >Word Alignment and the Expectation-Maximization Algorithm</a>
  tutorial (try <a href="https://pdfs.semanticscholar.org/8338/914856defebe908394c2b33dc43d350c5dd0.pdf?_ga=1.153336056.1178462967.1483354457" >here</a>) <br />
  <a href="http://mt-class.org/" >http://mt-class.org/</a><br />
  <a href="http://mttalks.ufal.ms.mff.cuni.cz/index.php?title=Main_Page" >MT Talks</a>
</td>
<td><a href="assignments/anlp16-finalproject.pdf" >final project guidelines</a></td>
</tr>
<tr>
<td>F 1/6</td>
<td><i>discussion of A4</i></td>
<td><span style="color: #2288aa;">be prepared to present your solutions!</span></td>
<td><a href="assignments/anlp16-assignment-5.pdf" >A5 released</a></td>
</tr>
<tr>
<td>T 1/10</td>
<td>Phrase-based machine translation</td>
<td>Jurafsky/Martin, ch. 25</td>
<td></td>
</tr>
<tr>
<td>F 1/13</td>
<td>Syntax-based machine translation</td>
<td>David Chiang, <a href="http://www.mitpressjournals.org/doi/pdf/10.1162/coli.2007.33.2.201" >Hierarchical phrase-based translation</a>. Computational Linguistics, 2007.</td>
<td><span style="color: #ff0000;">project proposal due!</span></td>
</tr>
<tr>
<td>T 1/17</td>
<td>Semantic parsing</td>
<td>Zettlemoyer/Collins, <a href="http://www.cs.columbia.edu/~mcollins/papers/uai05.pdf" >Learning to Map Sentences to
  Logical Form. Structured Classification with Probabilistic
  Categorial Grammars</a>. 2005 <br />
Wong/Mooney, <a href="http://www.cs.utexas.edu/~ml/papers/wasp-naacl-06.pdf" >Learning for Semantic Parsing with
  Statistical Machine Translation</a>. HLT-NAACL, 2006 <br />
Opt.: Mark Steedman, <a href="http://www.inf.ed.ac.uk/teaching/courses/nlg/readings/ccgintro.pdf" >A very short introduction to CCG</a>. 1996 </td>
<td></td>
</tr>
<tr>
<td>F 1/20</td>
<td>Lexical semantics </td>
<td>Jurafsky/Martin, chs. 19 + 20 <br />
Further reading:  Mitchell/Lapata (2008), <a href="http://anthology.aclweb.org/P/P08/P08-1028.pdf" >Vector-based models of
  semantic composition</a>; 
Baroni/Zamparelli (2010), <a href="http://www.aclweb.org/anthology/D/D10/D10-1115.pdf" >Nouns are vectors, adjectives are matrices</a></td>
<td>A5 due, <a href="assignments/anlp16-assignment-6.pdf">A6 released</a></td>
</tr>
<tr>
<td>T 1/24</td>
<td>LDA</td>
<td>David Blei, <a href="http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf" >Probabilistic topic models</a>. 2012</td>
<td></td>
</tr>
<tr>
<td>F 1/27</td>
<td><i>discussion of A5</i></td>
<td><span style="color: #2288aa;">be prepared to present your solutions!</span></td>
<td></td>
</tr>
<tr>
<td>T 1/31</td>
<td>NLP for discourse</td>
<td>Webber/Egg/Kordoni, <a
  href="https://doi.org/10.1017/S1351324911000337" >Discourse
  Structure and Language Technology</a>. 2012. <i>(accessible from the
U Potsdam network)</i><br />
We'll discuss the following papers on Bayesian pragmatics for
  discourse relations:<br />
<a href="https://www.aclweb.org/anthology/P/P16/P16-2086.pdf" >Yung et al., 2016a</a>, <a href="https://aclweb.org/anthology/K/K16/K16-1030.pdf" >Yung et al., 2016b</a>
</td>
<td></td>
</tr>
<tr>
<td>F 2/3</td>
<td><span style="color: #ff0000;">no class (instructor away)</span></td>
<td></td>
<td>A6 due</td>
</tr>
<tr>
<td>T 2/7</td>
<td>Intro to deep neural networks</td>
<td>Richard Socher,<a href="http://nlp.stanford.edu/~socherr/thesis.pdf" > Recursive Deep Learning for Natural Language
  Processing and Computer Vision</a> (Chapter 2). 2014<br />
Further reading (for intuitions): <a
  href="http://karpathy.github.io/neuralnets/"
  >http://karpathy.github.io/neuralnets/</a>, <a href="http://neuralnetworksanddeeplearning.com/chap4.html" >http://neuralnetworksanddeeplearning.com/chap4.html</a></td>
<td></td>
</tr>
<tr>
<td>F 2/10</td>
<td><i>discussion of A6</i> + <i>presentations of final projects</i></td>
<td><span style="color: #2288aa;">10-minute presentations of ideas,
  approaches, preliminary results</span></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="container">

<h2>Literature</h2>

<ul>
  <li>Dan Jurafsky and James Martin, <i>Speech and Language
  Processing</i>. (2008 or 2013 edition)</li>
  <li>Chris Manning and Hinrich Sch端tze, <i>Foundations of Statistical Natural Language Processing</i>. </li>
</ul>
<p>
Most computational linguists own both of these books. We will assign weekly
readings, so you should ensure you get your own copy or have access to
the copies that are available in the university library.
</p>

<ul>
  <li><i>Natural Language Processing with Python</i>. (the NLTK book)
  Available <a href="http://www.nltk.org/book_1ed/" >here</a>.</li>
</ul>


</div>

<div class="foot">
<!-- hhmts start -->Last modified: Tue Apr  4 09:34:36 CEST 2017 <!-- hhmts end -->
</div>
</body>
</html>
